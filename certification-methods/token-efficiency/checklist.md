# Token Efficiency - Evaluation Checklist

Use this checklist for every evaluation:

## Pre-Analysis

- [ ] Classify Prompt Complexity (Simple / Moderate / Complex)
- [ ] Define Ideal Output Profile (Target length, structure, density)

## Core Measurements

- [ ] Count Actual Output Tokens
- [ ] Conduct Redundancy Detection (Semantic overlap, not just wording)
- [ ] Measure Information Density (Distinct points per token)
- [ ] Run Compression Resistance Test (DEFLATE, Brotli, etc.)
- [ ] Perform Entropy Analysis (Shannon bits per token)

## Comparative Evaluation

- [ ] Calculate Deviation from Ideal Profile
- [ ] Adjust Contextually (industry, user sophistication)

## Quality Control

- [ ] Run Plausibility and Logic Testing
- [ ] Conduct Audience Sensitivity Analysis
- [ ] Apply Adaptive Threshold Learning (if past data available)

## Final Scoring

- [ ] Assign Token Efficiency Result (Platinum if within ±15%, Gold if within ±30%, Fail otherwise)

---

# Notes

- Every failure must be explained precisely.
- Context and audience always modify final judgments.
- Data must be saved for long-term learning (Adaptive Thresholding).
